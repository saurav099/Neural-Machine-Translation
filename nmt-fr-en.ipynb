{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\nimport random\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras import Sequential","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## Reading the Data","metadata":{}},{"cell_type":"code","source":"DF = pd.read_csv(\"../input/language-translation-englishfrench/eng_-french.csv\")","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Viewing the datasest","metadata":{}},{"cell_type":"code","source":"DF","metadata":{"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                                  English words/sentences  \\\n0                                                     Hi.   \n1                                                    Run!   \n2                                                    Run!   \n3                                                    Who?   \n4                                                    Wow!   \n...                                                   ...   \n175616  Top-down economics never works, said Obama. \"T...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                   French words/sentences  \n0                                                  Salut!  \n1                                                 Cours !  \n2                                                Courez !  \n3                                                   Qui ?  \n4                                              Ça alors !  \n...                                                   ...  \n175616  « L'économie en partant du haut vers le bas, ç...  \n175617  Une empreinte carbone est la somme de pollutio...  \n175618  La mort est une chose qu'on nous décourage sou...  \n175619  Puisqu'il y a de multiples sites web sur chaqu...  \n175620  Si quelqu'un qui ne connaît pas vos antécédent...  \n\n[175621 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English words/sentences</th>\n      <th>French words/sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. \"T...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n  </tbody>\n</table>\n<p>175621 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Renaming the columns of DataFrame","metadata":{}},{"cell_type":"code","source":"DF = DF.rename(columns={'English words/sentences': 'English', 'French words/sentences': 'French'})\nDF","metadata":{"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                                                  English  \\\n0                                                     Hi.   \n1                                                    Run!   \n2                                                    Run!   \n3                                                    Who?   \n4                                                    Wow!   \n...                                                   ...   \n175616  Top-down economics never works, said Obama. \"T...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                                   French  \n0                                                  Salut!  \n1                                                 Cours !  \n2                                                Courez !  \n3                                                   Qui ?  \n4                                              Ça alors !  \n...                                                   ...  \n175616  « L'économie en partant du haut vers le bas, ç...  \n175617  Une empreinte carbone est la somme de pollutio...  \n175618  La mort est une chose qu'on nous décourage sou...  \n175619  Puisqu'il y a de multiples sites web sur chaqu...  \n175620  Si quelqu'un qui ne connaît pas vos antécédent...  \n\n[175621 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>French</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. \"T...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n  </tbody>\n</table>\n<p>175621 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-Processing of the Data","metadata":{}},{"cell_type":"markdown","source":"## Separating the dataset into English and French","metadata":{}},{"cell_type":"code","source":"english = DF.English\nenglish","metadata":{"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"0                                                       Hi.\n1                                                      Run!\n2                                                      Run!\n3                                                      Who?\n4                                                      Wow!\n                                ...                        \n175616    Top-down economics never works, said Obama. \"T...\n175617    A carbon footprint is the amount of carbon dio...\n175618    Death is something that we're often discourage...\n175619    Since there are usually multiple websites on a...\n175620    If someone who doesn't know your background sa...\nName: English, Length: 175621, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"french = DF.French\nfrench","metadata":{"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"0                                                    Salut!\n1                                                   Cours !\n2                                                  Courez !\n3                                                     Qui ?\n4                                                Ça alors !\n                                ...                        \n175616    « L'économie en partant du haut vers le bas, ç...\n175617    Une empreinte carbone est la somme de pollutio...\n175618    La mort est une chose qu'on nous décourage sou...\n175619    Puisqu'il y a de multiples sites web sur chaqu...\n175620    Si quelqu'un qui ne connaît pas vos antécédent...\nName: French, Length: 175621, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing the English DataFrame","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(english):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    english[i] = stri","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Printing the first Ten Tokenized words of English DataFrame","metadata":{}},{"cell_type":"code","source":"print(english[0:10])","metadata":{"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"0      hi \n1     run \n2     run \n3     who \n4     wow \n5    fire \n6    help \n7    jump \n8    stop \n9    stop \nName: English, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizing the French DataFrame","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(french):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    french[i] = stri","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Printing the first Ten Tokenized words of French DataFrame","metadata":{}},{"cell_type":"code","source":"print(french[0:10])","metadata":{"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"0        salut \n1        cours \n2       courez \n3          qui \n4     ça alors \n5       au feu \n6     à l aide \n7        saute \n8    ça suffit \n9         stop \nName: French, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Transforming data into arrays","metadata":{}},{"cell_type":"code","source":"n1 = 0\nn2 = 100\neng = list(english)\nfre = list(french)\n\n# for DF in english:eng.append(DF)\n\n# for DF in french:fre.append(DF)\n\neng = np.asarray(eng)\nfre = np.asarray(fre)\n\neng = eng[0:175000]\nfre = fre[0:175000]\n\nfor i in range(n1,n2):\n  print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")","metadata":{"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"hi \t->\tsalut \n\nrun \t->\tcours \n\nrun \t->\tcourez \n\nwho \t->\tqui \n\nwow \t->\tça alors \n\nfire \t->\tau feu \n\nhelp \t->\tà l aide \n\njump \t->\tsaute \n\nstop \t->\tça suffit \n\nstop \t->\tstop \n\nstop \t->\tarrête toi \n\nwait \t->\tattends \n\nwait \t->\tattendez \n\ngo on \t->\tpoursuis \n\ngo on \t->\tcontinuez \n\ngo on \t->\tpoursuivez \n\nhello \t->\tbonjour \n\nhello \t->\tsalut \n\ni see \t->\tje comprends \n\ni try \t->\tj essaye \n\ni won \t->\tj ai gagné \n\ni won \t->\tje l ai emporté \n\ni won \t->\tj ai gagné \n\noh no \t->\toh non \n\nattack \t->\tattaque \n\nattack \t->\tattaquez \n\ncheers \t->\tsanté \n\ncheers \t->\tà votre santé \n\ncheers \t->\tmerci \n\ncheers \t->\ttchin tchin \n\nget up \t->\tlève toi \n\ngo now \t->\tva maintenant \n\ngo now \t->\tallez y maintenant \n\ngo now \t->\tvas y maintenant \n\ngot it \t->\tj ai pigé \n\ngot it \t->\tcompris \n\ngot it \t->\tpigé \n\ngot it \t->\tcompris \n\ngot it \t->\tt as capté \n\nhop in \t->\tmonte \n\nhop in \t->\tmontez \n\nhug me \t->\tserre moi dans tes bras \n\nhug me \t->\tserrez moi dans vos bras \n\ni fell \t->\tje suis tombée \n\ni fell \t->\tje suis tombé \n\ni know \t->\tje sais \n\ni left \t->\tje suis parti \n\ni left \t->\tje suis partie \n\ni lied \t->\tj ai menti \n\ni lost \t->\tj ai perdu \n\ni paid \t->\tj ai payé \n\ni m 19 \t->\tj ai 19 ans \n\ni m ok \t->\tje vais bien \n\ni m ok \t->\tça va \n\nlisten \t->\técoutez \n\nno way \t->\tc est pas possible \n\nno way \t->\timpossible \n\nno way \t->\ten aucun cas \n\nno way \t->\tsans façons \n\nno way \t->\tc est hors de question \n\nno way \t->\til n en est pas question \n\nno way \t->\tc est exclu \n\nno way \t->\ten aucune manière \n\nno way \t->\thors de question \n\nreally \t->\tvraiment \n\nreally \t->\tvrai \n\nreally \t->\tah bon \n\nthanks \t->\tmerci \n\nwe try \t->\ton essaye \n\nwe won \t->\tnous avons gagné \n\nwe won \t->\tnous gagnâmes \n\nwe won \t->\tnous l avons emporté \n\nwe won \t->\tnous l emportâmes \n\nask tom \t->\tdemande à tom \n\nawesome \t->\tfantastique \n\nbe calm \t->\tsois calme \n\nbe calm \t->\tsoyez calme \n\nbe calm \t->\tsoyez calmes \n\nbe cool \t->\tsois détendu \n\nbe fair \t->\tsois juste \n\nbe fair \t->\tsoyez juste \n\nbe fair \t->\tsoyez justes \n\nbe fair \t->\tsois équitable \n\nbe fair \t->\tsoyez équitable \n\nbe fair \t->\tsoyez équitables \n\nbe kind \t->\tsois gentil \n\nbe nice \t->\tsois gentil \n\nbe nice \t->\tsois gentille \n\nbe nice \t->\tsoyez gentil \n\nbe nice \t->\tsoyez gentille \n\nbe nice \t->\tsoyez gentils \n\nbe nice \t->\tsoyez gentilles \n\nbeat it \t->\tdégage \n\ncall me \t->\tappelle moi \n\ncall me \t->\tappellez moi \n\ncall us \t->\tappelle nous \n\ncall us \t->\tappelez nous \n\ncome in \t->\tentrez \n\ncome in \t->\tentre \n\ncome in \t->\tentre \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Counting English and French Words","metadata":{}},{"cell_type":"code","source":"e = [word for sentence in eng for word in sentence.split(\" \")]\nf = [word for sentence in fre for word in sentence.split(\" \")]\nenglish_word_counter = collections.Counter(e)\nfrench_word_counter = collections.Counter(f)","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"print('{} English words.'.format(len(e)))\nprint('{} French words.'.format(len(f)))\nprint(\"\\n\")\nprint('{} unique English words.'.format(len(english_word_counter)))\nprint('{} unique French words.'.format(len(french_word_counter)))\nprint(\"\\n\")\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_word_counter.most_common(10)))[0]) + '\"')\nprint(\"\\n\")\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_word_counter.most_common(10)))[0]) + '\"')","metadata":{"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"1308720 English words.\n1425733 French words.\n\n\n13917 unique English words.\n23918 unique French words.\n\n\n10 Most common words in the English dataset:\n\"\" \"i\" \"you\" \"to\" \"the\" \"a\" \"t\" \"is\" \"that\" \"tom\"\n\n\n10 Most common words in the French dataset:\n\"\" \"je\" \"de\" \"pas\" \"est\" \"vous\" \"que\" \"il\" \"à\" \"ne\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sorting the above data into table form","metadata":{}},{"cell_type":"code","source":"dict1 = {1: [\"English \", 1133720, 13917 ], \n     2: [\"French\", 1250733, 23918] \n     } \n# Print the names of the columns. \nprint (\"{:<15} {:<15} {:<15}\".format('LANGUAGE', 'TOTAL WORDS', 'UNIQUE WORDS')) \n  \n# print each data item. \nfor key, value in dict1.items(): \n    language, total_words, unique_words = value \n    print (\"{:<15} {:<15} {:<15}\".format(language, total_words, unique_words)) ","metadata":{"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"LANGUAGE        TOTAL WORDS     UNIQUE WORDS   \nEnglish         1133720         13917          \nFrench          1250733         23918          \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Indexing of a smaple text with help of tokenization","metadata":{}},{"cell_type":"code","source":"def tokenize(x):\n    tokenizer = Tokenizer(char_level=False,oov_token=\" \")\n    tokenizer.fit_on_texts(x)\n    return tokenizer.texts_to_sequences(x), tokenizer\n\ntext_sentences = [\n    'An apple a day keeps a doctor away .',\n    'well, hope this letter of mine finds u in pink of your health .',\n    'This is a short sentence .']\n  \ntext , tokenizer = tokenize(text_sentences)\nprint(text)\nprint(tokenizer.word_index)","metadata":{"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"[[5, 6, 2, 7, 8, 2, 9, 10], [11, 12, 3, 13, 4, 14, 15, 16, 17, 18, 4, 19, 20], [3, 21, 2, 22, 23]]\n{' ': 1, 'a': 2, 'this': 3, 'of': 4, 'an': 5, 'apple': 6, 'day': 7, 'keeps': 8, 'doctor': 9, 'away': 10, 'well': 11, 'hope': 12, 'letter': 13, 'mine': 14, 'finds': 15, 'u': 16, 'in': 17, 'pink': 18, 'your': 19, 'health': 20, 'is': 21, 'short': 22, 'sentence': 23}\n","output_type":"stream"}]},{"cell_type":"code","source":"def pad(x,length=None):\n    if (length==None):\n        length = max([len(sentence) for sentence in x])\n    a = pad_sequences(x,maxlen=length,padding=\"post\")\n    return a\n\n","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## Indexing of complete Dataset","metadata":{}},{"cell_type":"code","source":"def preprocess(x,y):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_y,y_tk = tokenize(y)\n\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n    print(*preprocess_y.shape)\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\ndef preprocessing(x):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_x = pad(preprocess_x)\n    return preprocess_x, x_tk","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessed Information about the data","metadata":{}},{"cell_type":"code","source":"pre_eng,pre_fre,eng_tk,fre_tk = preprocess(eng,fre)\nmax_eng_seq_len = pre_eng.shape[1]\nmax_fr_seq_len = pre_fre.shape[1]\nenglish_vocab_size = len(eng_tk.word_index)\nfrench_vocab_size = len(fre_tk.word_index)\n\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_eng_seq_len)\nprint(\"Max French sentence length:\", max_fr_seq_len)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","metadata":{"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"175000 26\nData Preprocessed\nMax English sentence length: 21\nMax French sentence length: 26\nEnglish vocabulary size: 13917\nFrench vocabulary size: 23918\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Implementation ","metadata":{}},{"cell_type":"code","source":"def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n\n    learning_rate = 0.001\n    model = keras.Sequential([\n                                Embedding(english_vocab_size+1, \n                                          128, \n                                          input_length = input_shape[1]),\n        \n                                Bidirectional(GRU(128, \n                                                  return_sequences=True)),\n        \n                                tf.keras.layers.Dropout(0.25),\n        \n                                TimeDistributed(Dense(french_vocab_size, \n                                                      activation='softmax'))\n                                ])\n    model.summary()\n    \n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n\ntmp_x = pad(pre_eng, \n            max_fr_seq_len)\n\nrnn_model = embed_model(tmp_x.shape,\n                        max_fr_seq_len,\n                        english_vocab_size,\n                        french_vocab_size)\n\nrnn_model.fit(tmp_x, pre_fre, batch_size=1024, epochs=20, validation_split=0.2)\n\nrnn_model.save_weights(\"rnn_model_weights.h5\")","metadata":{"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 26, 128)           1781504   \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 26, 256)           198144    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 26, 256)           0         \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, 26, 23918)         6146926   \n=================================================================\nTotal params: 8,126,574\nTrainable params: 8,126,574\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/20\n137/137 [==============================] - 76s 539ms/step - loss: 5.1662 - accuracy: 0.7268 - val_loss: nan - val_accuracy: 0.5927\nEpoch 2/20\n137/137 [==============================] - 73s 534ms/step - loss: 1.6017 - accuracy: 0.7684 - val_loss: nan - val_accuracy: 0.5934\nEpoch 3/20\n137/137 [==============================] - 73s 535ms/step - loss: 1.5570 - accuracy: 0.7708 - val_loss: nan - val_accuracy: 0.6024\nEpoch 4/20\n137/137 [==============================] - 73s 534ms/step - loss: 1.4909 - accuracy: 0.7825 - val_loss: nan - val_accuracy: 0.6196\nEpoch 5/20\n137/137 [==============================] - 73s 534ms/step - loss: 1.3912 - accuracy: 0.7935 - val_loss: nan - val_accuracy: 0.6297\nEpoch 6/20\n137/137 [==============================] - 73s 535ms/step - loss: 1.2883 - accuracy: 0.8032 - val_loss: nan - val_accuracy: 0.6375\nEpoch 7/20\n137/137 [==============================] - 73s 535ms/step - loss: 1.2054 - accuracy: 0.8099 - val_loss: nan - val_accuracy: 0.6446\nEpoch 8/20\n137/137 [==============================] - 74s 537ms/step - loss: 1.1368 - accuracy: 0.8156 - val_loss: nan - val_accuracy: 0.6496\nEpoch 9/20\n137/137 [==============================] - 73s 535ms/step - loss: 1.0763 - accuracy: 0.8202 - val_loss: nan - val_accuracy: 0.6553\nEpoch 10/20\n137/137 [==============================] - 73s 536ms/step - loss: 1.0261 - accuracy: 0.8236 - val_loss: nan - val_accuracy: 0.6594\nEpoch 11/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.9810 - accuracy: 0.8267 - val_loss: nan - val_accuracy: 0.6628\nEpoch 12/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.9435 - accuracy: 0.8292 - val_loss: nan - val_accuracy: 0.6655\nEpoch 13/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.9123 - accuracy: 0.8314 - val_loss: nan - val_accuracy: 0.6676\nEpoch 14/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.8792 - accuracy: 0.8340 - val_loss: nan - val_accuracy: 0.6696\nEpoch 15/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.8525 - accuracy: 0.8361 - val_loss: nan - val_accuracy: 0.6715\nEpoch 16/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.8271 - accuracy: 0.8383 - val_loss: nan - val_accuracy: 0.6738\nEpoch 17/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.8063 - accuracy: 0.8401 - val_loss: nan - val_accuracy: 0.6759\nEpoch 18/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.7877 - accuracy: 0.8420 - val_loss: nan - val_accuracy: 0.6775\nEpoch 19/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.7691 - accuracy: 0.8439 - val_loss: nan - val_accuracy: 0.6789\nEpoch 20/20\n137/137 [==============================] - 73s 535ms/step - loss: 0.7498 - accuracy: 0.8463 - val_loss: nan - val_accuracy: 0.6798\n","output_type":"stream"}]},{"cell_type":"code","source":"def logits_to_text(logits, tokenizer):\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","metadata":{"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## Checking for translation ","metadata":{}},{"cell_type":"code","source":"for k in range(10):\n    predicted=[]\n    a = random.randint(0,100000)\n    print('Random Index: ', a)\n\n    print(\"PREDICTED:\\t\", end=' ')\n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n        print(x, end =' ')\n        if x!='<PAD>':\n            predicted.append(x)\n\n    \n    english = eng[a].split()\n    french = fre[a].split()\n\n    print(\"\\n\\nENGLISH:\\t\", eng[a] + \"\\nFRENCH:\\t\\t \" + fre[a] + \"\\n\")\n    print('\\nIndexing: ',tmp_x[a])\n    \n    print('\\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\\n')","metadata":{"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Random Index:  86482\nPREDICTED:\t tom adhésif père fox très \n\nENGLISH:\t tom s father is very strict \nFRENCH:\t\t le père de tom est très sévère \n\n\nIndexing:  [  10   13  169    8   55 2266    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  90972\nPREDICTED:\t j honnête non sûre j \n\nENGLISH:\t i m not sure i can trust you \nFRENCH:\t\t je ne suis pas sûre que je puisse me fier à vous \n\n\nIndexing:  [  2  28  34 118   2  25 421   3   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  74070\nPREDICTED:\t saviez tu dis trente aida \n\nENGLISH:\t did you say thirty euros \nFRENCH:\t\t est ce que tu as dit trente euros \n\n\nIndexing:  [  42    3  114  628 3857    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  72384\nPREDICTED:\t nous aida personne pierre oh \n\nENGLISH:\t we left no stone unturned \nFRENCH:\t\t nous n avons épargné aucun effort \n\n\nIndexing:  [  21  174   59 1284 3935    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  4437\nPREDICTED:\t cette fox laid <PAD> <PAD> \n\nENGLISH:\t this is ugly \nFRENCH:\t\t c est hideux \n\n\nIndexing:  [  16    8 1631    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  40506\nPREDICTED:\t il fox aussi bons aussi \n\nENGLISH:\t he is as good as dead \nFRENCH:\t\t il est comme mort \n\n\nIndexing:  [ 12   8  61  77  61 523   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  52994\nPREDICTED:\t acceptez tu réfléchissez j honnête \n\nENGLISH:\t do you think i m stupid \nFRENCH:\t\t vous me prenez pour un idiot \n\n\nIndexing:  [ 14   3  46   2  28 423   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  82888\nPREDICTED:\t j devez personne promit aider \n\nENGLISH:\t i have no objection to that \nFRENCH:\t\t je n y vois pas d objection \n\n\nIndexing:  [   2   20   59 3235    4    9    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  58455\nPREDICTED:\t pourquoi êtes tu alors souviens \n\nENGLISH:\t why are you so insecure \nFRENCH:\t\t pourquoi es tu si anxieux \n\n\nIndexing:  [  65   27    3   75 3055    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\nRandom Index:  79914\nPREDICTED:\t quelles irez tu acceptez si \n\nENGLISH:\t what ll you do if it rains \nFRENCH:\t\t que feras tu s il pleut \n\n\nIndexing:  [  23   47    3   14   66   11 1214    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0]\n\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CALCULATING BLEU SCORE","metadata":{}},{"cell_type":"code","source":"n = []\nlst=[]\nk=0\nwhile k<1000:\n    \n    predicted=[]\n    a = random.randint(0,175000)\n    \n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n    \n        if x!='<PAD>':\n            predicted.append(x)\n\n    english = eng[a].split()\n    french = fre[a].split()\n\n    references = [[french]]\n    candidates = [predicted]\n    score = corpus_bleu(references, candidates, weights=(0.05, 0.25, 0.35, 0.35))\n    lst.append(score)\n    if score>.8:\n        n+=[score]\n    k+=1\n    \n    if k%100==0:\n        print(k)        \ndef average(lst):\n    print(sum(lst)/len(lst))   \naverage(n)","metadata":{"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n0.9409652607509906\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"AVERAGE BLEU SCORE: \", end ='\\t') \naverage(n)\n","metadata":{"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"AVERAGE BLEU SCORE: \t0.9409652607509906\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---","metadata":{}}]}